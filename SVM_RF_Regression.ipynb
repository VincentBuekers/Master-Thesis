{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Support Vector Machine Embedded Random Forest Regressor\n",
    "## Thesis: Master of statistics, KUL\n",
    "### Vincent Buekers\n",
    "Promotor: Prof. dr. Johan A.K. Suykens\n",
    "\n",
    "Supervision: Yingyi Chen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets, tree, svm, metrics, pipeline, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import multiprocessing as multip\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition input space into subsets \n",
    "Subsets are obtained from the leaf nodes of an extremely randomized tree. For purposes of theoretical consistency, only one candidate feature is selected from all d features using the option max_features = 1, yielding totally random trees.\n",
    "\n",
    "Note: these are non-overlapping subsets due to the recursive branching mechanism in decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_groups(X,y, X_train,X_test, y_train,y_test, idx_train,idx_test):\n",
    "    \n",
    "    subsets = []\n",
    "        \n",
    "    # totally randomized tree (max_features=1)\n",
    "    extra = tree.ExtraTreeRegressor(max_features=1, min_samples_leaf = int(np.sqrt(len(X))) )\n",
    "    extra.fit(X_train,y_train)\n",
    "    \n",
    "    # obtain leaf indices the datapoints appear in\n",
    "    leaf_idx_train, leaf_idx_test = extra.apply(X_train), extra.apply(X_test)\n",
    "    \n",
    "    # Keep track of observation indexes and prepare for pandas' .groupby\n",
    "    leaf_idx_train = pd.DataFrame(leaf_idx_train, index=idx_train)\n",
    "    leaf_idx_test = pd.DataFrame(leaf_idx_test, index=idx_test)\n",
    "    \n",
    "    # Group train and test observations by their leaf node\n",
    "    groups_train = leaf_idx_train.groupby(leaf_idx_train[0],axis=0).groups\n",
    "    groups_test = leaf_idx_test.groupby(leaf_idx_test[0],axis=0).groups\n",
    "        \n",
    "    # Obtain train and test subsets created by the leaf node partitioning\n",
    "    # iterables are a list of Int64index objects for the data in each leaf node\n",
    "    for value_train, value_test in zip(list(groups_train.values()),list(groups_test.values())) :\n",
    "        \n",
    "        # subset the data\n",
    "        X_train_sub, y_train_sub = X[value_train], y[value_train]\n",
    "        X_test_sub, y_test_sub = X[value_test] ,y[value_test]\n",
    "        \n",
    "        # original indexes of the observations appearing in this leaf\n",
    "        train_indexes = np.array(value_train).reshape(-1,1)\n",
    "        test_indexes = np.array(value_test).reshape(-1,1)\n",
    "        \n",
    "        # training subset including original observation indexes\n",
    "        sub_train = np.concatenate((train_indexes, X_train_sub, y_train_sub), axis=1)\n",
    "        # testing subset including original observation indexes and tree predictions\n",
    "        sub_test = np.concatenate((test_indexes, X_test_sub, y_test_sub), axis=1)\n",
    "\n",
    "        subsets.append([sub_train,sub_test])\n",
    "    \n",
    "    return subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedded SVM Regressor\n",
    "for each subset an svm classifier is trained on the training subset and used to predict the corresponding leaf test test.\n",
    "\n",
    "Since support vector machines are not scale-invariant, the data are scaled to have zero mean and unit variance for each local SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_svm(subset):\n",
    "    \n",
    "    idx_train, X_train, y_train = subset[0][:,0], subset[0][:,1:-1], subset[0][:,-1]\n",
    "    idx_test,X_test,y_test = subset[1][:,0],subset[1][:,1:-1],subset[1][:,-1]\n",
    "    \n",
    "    # preprocess data\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "        \n",
    "    # decide whether to solve in primal or dual\n",
    "    QP_bool = False if (X_train.shape[0] > X_train.shape[0]) else True\n",
    "        \n",
    "    # fit svm to subset\n",
    "    reg = svm.LinearSVR(dual=QP_bool)\n",
    "    reg.fit(X_train,y_train)\n",
    "    \n",
    "    # fit svm to subset\n",
    "    #reg = svm.SVR(kernel=\"rbf\")\n",
    "    #pipe = pipeline.Pipeline([('Scaler', preprocessing.StandardScaler()), ('svr', reg)])\n",
    "    #pipe.fit(X_train,y_train)\n",
    "        \n",
    "    # obtain predictions for subset\n",
    "    svm_pred = reg.predict(X_test)\n",
    "    svm_pred = np.concatenate((idx_test.reshape(-1,1), svm_pred.reshape(-1,1)), axis=1)\n",
    "        \n",
    "    return svm_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_parallel(X,y, X_train,X_test, y_train,y_test, idx_train,idx_test):\n",
    "    \n",
    "    # Obtain subsets\n",
    "    subsets = get_groups(X,y, X_train,X_test, y_train,y_test, idx_train,idx_test)\n",
    "    \n",
    "    preds_leaf = []\n",
    "    \n",
    "    # Run SVM's in parallel\n",
    "    with Parallel() as parallel:\n",
    "        result = parallel(delayed(fit_svm)(subset) for subset in subsets)\n",
    "        preds_leaf.append(result)\n",
    "        \n",
    "    # aggregate predictions of the leafs into one set of predictions for the tree\n",
    "    preds_all = np.concatenate(preds_leaf[0],axis=0)\n",
    "    \n",
    "    # sort predictions by their index\n",
    "    preds_sorted = preds_all[np.argsort(preds_all[:,0])]\n",
    "    \n",
    "    return preds_sorted[:,1].reshape(-1,1) # return predictions without index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialized Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def svm_tree(X,y, X_train,X_test, y_train,y_test, idx_train,idx_test):\n",
    "    \n",
    "    subsets = get_groups(X,y, X_train,X_test, y_train,y_test, idx_train,idx_test)\n",
    "    \n",
    "    preds_leaf = []\n",
    "    \n",
    "    for subset in subsets:\n",
    "        \n",
    "        # Train data stored in first element of each leaf (subset)\n",
    "        idx_train, X_train, y_train = subset[0][:,0], subset[0][:,1:-1], subset[0][:,-1]\n",
    "        # test data  stored in second element of each leaf (subset)\n",
    "        idx_test, X_test, y_test = subset[1][:,0], subset[1][:,1:-1], subset[1][:,-1]\n",
    "        \n",
    "        # svm classifier with Radial basis function kernel\n",
    "        reg = svm.SVR(kernel='rbf')\n",
    "        # fit svm to subset\n",
    "        reg.fit(X_train,y_train)\n",
    "            \n",
    "        # obtain predictions for subset\n",
    "        svm_pred = reg.predict(X_test)\n",
    "        # keep track of observaton indexes\n",
    "        svm_pred = np.concatenate((idx_test.reshape(-1,1), svm_pred.reshape(-1,1)), axis=1)\n",
    "        preds_leaf.append(svm_pred)\n",
    "\n",
    "    # aggregate leaf predictions for all test samples across different nodes\n",
    "    preds_all = np.concatenate(preds_leaf,axis=0)\n",
    "    # sort predictions by their index\n",
    "    preds_sorted = preds_all[np.argsort(preds_all[:,0])]\n",
    "    return preds_sorted[:,1].reshape(-1,1) # return predictions without index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the above procedure for multiple trees in parallel\n",
    "This results in a support vector machine embedded random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run trees in parallel using all cores\n",
    "def svm_forest(X,y):\n",
    "    \n",
    "    # amount of trees in forest\n",
    "    trees = 100\n",
    "    \n",
    "    forest_pred=[]\n",
    "    \n",
    "    X_train,X_test, y_train,y_test, idx_train,idx_test = train_test_split(X,y\n",
    "                                                                          ,np.arange(len(X))\n",
    "                                                                          ,test_size=1/3)\n",
    "    t = time.time()\n",
    "    \n",
    "    with Parallel() as parallel:\n",
    "        forest_pred = parallel(delayed(svm_parallel)(X,y \n",
    "                                                     ,X_train,X_test \n",
    "                                                     ,y_train,y_test\n",
    "                                                     ,idx_train,idx_test) for i in range(1,trees))\n",
    "    \n",
    "    # training time\n",
    "    training_time = time.time() - t\n",
    "    \n",
    "    # reshape array such that column k denotes prediction for tree k\n",
    "    forest_pred = np.concatenate(forest_pred,axis=1)\n",
    "    \n",
    "    mean_pred = np.apply_along_axis(np.mean, 1, forest_pred) \n",
    "    \n",
    "    test = y_test[np.argsort(idx_test)]\n",
    "    \n",
    "    R_2 = metrics.r2_score(test, mean_pred)\n",
    "    \n",
    "    return R_2, training_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boston housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_2: 0.7673728699623378\n",
      "training time: 0.63 seconds\n"
     ]
    }
   ],
   "source": [
    "X, y = datasets.load_boston(return_X_y=True)\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "svm_rf = svm_forest(X,y)\n",
    "print(\"R_2: {}\".format(svm_rf[0]))\n",
    "print(\"training time: {0:.2f} seconds\".format(svm_rf[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# California Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_2: 0.5985182046186897\n",
      "training time: 95.49 seconds\n"
     ]
    }
   ],
   "source": [
    "X, y = datasets.fetch_california_housing(return_X_y=True)\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "svm_rf = svm_forest(X,y)\n",
    "print(\"R_2: {}\".format(svm_rf[0]))\n",
    "print(\"training time: {0:.2f} seconds\".format(svm_rf[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
