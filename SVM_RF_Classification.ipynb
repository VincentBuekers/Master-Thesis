{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Support Vector Machine Embedded Random Forest Classifier\n",
    "## Thesis: Master of statistics, KUL\n",
    "### Vincent Buekers\n",
    "Promotor: Prof. dr. Johan A.K. Suykens\n",
    "\n",
    "Supervision: Yingyi Chen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets, tree, svm, metrics, preprocessing, pipeline\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Vincent/Desktop/Thesis/Python/Data\n"
     ]
    }
   ],
   "source": [
    "%cd '/Users/Vincent/Desktop/Thesis/Python/Data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition input space into subsets \n",
    "Subsets are obtained from the leaf nodes of an extremely randomized tree. For purposes of theoretical consistency, only one candidate feature is selected from all d features using the option max_features = 1, yielding totally random trees.\n",
    "\n",
    "Note: these are non-overlapping subsets due to the recursive branching mechanism in decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_groups(X,y, X_train,X_test, y_train,y_test, idx_train,idx_test):\n",
    "    \n",
    "    subsets = []\n",
    "        \n",
    "    # totally randomized tree (max_features=1)\n",
    "    extra = tree.ExtraTreeClassifier(max_features=1,min_samples_leaf = int(np.sqrt(len(X))) )\n",
    "    extra.fit(X_train,y_train)\n",
    "    \n",
    "    # tree predictions\n",
    "    preds_tree = extra.predict(X).reshape(-1,1)\n",
    "    \n",
    "    # obtain leaf indices the datapoints appear in\n",
    "    leaf_idx_train, leaf_idx_test = extra.apply(X_train), extra.apply(X_test)\n",
    "    \n",
    "    # Keep track of observation indexes and prepare for pandas' .groupby\n",
    "    leaf_idx_train = pd.DataFrame(leaf_idx_train, index=idx_train)\n",
    "    leaf_idx_test = pd.DataFrame(leaf_idx_test, index=idx_test)\n",
    "    \n",
    "    # Group train and test observations by their leaf node\n",
    "    groups_train = leaf_idx_train.groupby(leaf_idx_train[0],axis=0).groups\n",
    "    groups_test = leaf_idx_test.groupby(leaf_idx_test[0],axis=0).groups\n",
    "        \n",
    "    # Obtain train and test subsets created by the leaf node partitioning\n",
    "    # iterables are a list of Int64index objects for the data in each leaf node\n",
    "    for value_train, value_test in zip(list(groups_train.values()),list(groups_test.values())) :\n",
    "        \n",
    "        # subset the data\n",
    "        X_train_sub, y_train_sub = X[value_train], y[value_train]\n",
    "        X_test_sub, y_test_sub, y_tree = X[value_test] ,y[value_test], preds_tree[value_test]\n",
    "        \n",
    "        # original indexes of the observations appearing in this leaf\n",
    "        train_indexes = np.array(value_train).reshape(-1,1)\n",
    "        test_indexes = np.array(value_test).reshape(-1,1)\n",
    "        \n",
    "        # training subset including original observation indexes\n",
    "        sub_train = np.concatenate((train_indexes, X_train_sub, y_train_sub), axis=1)\n",
    "        # testing subset including original observation indexes and tree predictions\n",
    "        sub_test = np.concatenate((test_indexes, X_test_sub, y_test_sub, y_tree), axis=1)\n",
    "\n",
    "        subsets.append([sub_train,sub_test])\n",
    "    \n",
    "    return subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedded SVM classifiers\n",
    "for each subset an svm classifier is trained on the training subset and used to predict the corresponding leaf test test (if the leaf is not yet homogenous in terms of class labels). \n",
    "\n",
    "For heterogeneous leafs, there might still be an issue with class imbalance since the forest partitioning tries aims at leaf purity. Therefore, the C parameter is automatically weighted inversely proportional to class frequencies.\n",
    "\n",
    "Since support vector machines are not scale-invariant, the data are scaled to have zero mean and unit variance for each local SVM \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Svm fitting and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_svm(subset):\n",
    "    \n",
    "    idx_train, X_train, y_train = subset[0][:,0], subset[0][:,1:-1], subset[0][:,-1]\n",
    "    idx_test,X_test,y_test,y_tree = subset[1][:,0],subset[1][:,1:-2],subset[1][:,-2],subset[1][:,-1]\n",
    "        \n",
    "    # check if leaf node is heterogeneous (i.e. consists of more than one class)\n",
    "    if (len(np.unique(y_train)) >= 2):\n",
    "        \n",
    "         # preprocess data\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        # decide whether to solve in primal or dual\n",
    "        QP_bool = False if (X_train.shape[0] > X_train.shape[0]) else True\n",
    "        \n",
    "        # fit svm to subset\n",
    "        clf = svm.LinearSVC(fit_intercept=False,class_weight='balanced', dual=QP_bool)\n",
    "        clf.fit(X_train,y_train)\n",
    "        \n",
    "        # fit svm to subset\n",
    "        #clf = svm.SVC(kernel=\"rbf\", class_weight='balanced')\n",
    "        #pipe = pipeline.Pipeline([('Scaler', preprocessing.StandardScaler()), ('svc', clf)])\n",
    "        #pipe.fit(X_train,y_train)\n",
    "        \n",
    "        # obtain predictions for subset\n",
    "        svm_pred = clf.predict(X_test)\n",
    "        svm_pred = np.concatenate((idx_test.reshape(-1,1), svm_pred.reshape(-1,1)), axis=1)\n",
    "        \n",
    "        return svm_pred\n",
    "    \n",
    "    # use tree predictions if leaf (subset) is already pure   \n",
    "    else:\n",
    "        tree_pred = np.concatenate((idx_test.reshape(-1,1), y_tree.reshape(-1,1)), axis=1)\n",
    "        \n",
    "        return tree_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concurrent execution of svms within tree\n",
    "Within each decision tree, the SVMs can be trained in parallel to achieve optimal computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_parallel(X,y, X_train,X_test, y_train,y_test, idx_train,idx_test):\n",
    "    # Obtain subsets\n",
    "    subsets = get_groups(X,y, X_train,X_test, y_train,y_test, idx_train,idx_test)\n",
    "    \n",
    "    preds_leaf = []\n",
    "    \n",
    "    # Run SVM's in parallel\n",
    "    with Parallel() as parallel:\n",
    "        result = parallel(delayed(fit_svm)(subset) for subset in subsets)\n",
    "        preds_leaf.append(result)\n",
    "        \n",
    "    # aggregate predictions of the leafs into one set of predictions for the tree\n",
    "    preds_all = np.concatenate(preds_leaf[0],axis=0)\n",
    "    \n",
    "    # sort predictions by their index\n",
    "    preds_sorted = preds_all[np.argsort(preds_all[:,0])]\n",
    "    \n",
    "    return preds_sorted[:,1].reshape(-1,1) # return predictions without index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def svm_tree(X,y, X_train,X_test, y_train,y_test, idx_train,idx_test):\n",
    "    \n",
    "    subsets = get_groups(X,y, X_train,X_test, y_train,y_test, idx_train,idx_test)\n",
    "    \n",
    "    preds_leaf = []\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for subset in subsets:\n",
    "        \n",
    "        # Train data stored in first element of each leaf (subset)\n",
    "        idx_train, X_train, y_train = subset[0][:,0], subset[0][:,1:-1], subset[0][:,-1]\n",
    "        # test data  stored in second element of each leaf (subset)\n",
    "        idx_test,X_test,y_test,y_tree = subset[1][:,0],subset[1][:,1:-2],subset[1][:,-2],subset[1][:,-1]\n",
    "        \n",
    "        # check if leaf node is heterogeneous (i.e. consists of more than one class)\n",
    "        if (len(np.unique(y_train)) >= 2):\n",
    "            \n",
    "            # svm classifier with Radial basis function kernel\n",
    "            clf = svm.SVC(kernel='rbf')\n",
    "            pipe = pipeline.Pipeline([('Scaler', preprocessing.StandardScaler()), ('svc', clf)])\n",
    "            # fit svm to subset\n",
    "            pipe.fit(X_train,y_train)\n",
    "            \n",
    "            # obtain predictions for subset\n",
    "            svm_pred = pipe.predict(X_test)\n",
    "            # keep track of observaton indexes\n",
    "            svm_pred = np.concatenate((idx_test.reshape(-1,1), svm_pred.reshape(-1,1)), axis=1)\n",
    "            preds_leaf.append(svm_pred)\n",
    "            \n",
    "            count += 1\n",
    "        \n",
    "        # use tree predictions if leaf (subset) is already pure   \n",
    "        else:\n",
    "            tree_pred = np.concatenate((idx_test.reshape(-1,1), y_tree.reshape(-1,1)), axis=1)\n",
    "            preds_leaf.append(tree_pred)\n",
    "    \n",
    "    print(\"The input space has been partitioned into {} leaf nodes, i.e. samples.\".format(len(subsets)))\n",
    "\n",
    "    print(\"{} of which have been used for training an SVM\"\\\n",
    "          \" given the heterogeneity of those nodes.\".format(count))\n",
    "    print(\"The remaining {} samples have the inherited\"\\\n",
    "          \" prediction from the randomized tree.\".format(len(subsets)-count))\n",
    "    \n",
    "    # aggregate leaf predictions for all test samples across different nodes\n",
    "    preds_all = np.concatenate(preds_leaf,axis=0)\n",
    "    # sort predictions by their index\n",
    "    preds_sorted = preds_all[np.argsort(preds_all[:,0])]\n",
    "    \n",
    "    return preds_sorted[:,1].reshape(-1,1) # return predictions without index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Majority Vote\n",
    "For classification, a majority vote is implemented to ultimately obtain the forest prediction, as is commonmy done in ensemble methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode, StatisticsError\n",
    "\n",
    "# Obtain majority vote for each datapoint\n",
    "def majority_vote(l):\n",
    "    try:\n",
    "        return mode(l)\n",
    "    except StatisticsError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM embedded Forest\n",
    "Finally, the above procedure can be extended to an ensemble of randomized trees with embedded SVMs. Apart from the parallel execution of the embedded svm predictors, the forest ensemble itself can also be concurrently executed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run trees in parallel using all cores\n",
    "def svm_forest(X,y):\n",
    "    \n",
    "    trees = 100\n",
    "    \n",
    "    forest_pred=[]\n",
    "    \n",
    "    X_train,X_test, y_train,y_test, idx_train,idx_test = train_test_split(X,y\n",
    "                                                                          ,np.arange(len(X))\n",
    "                                                                       ,test_size=1/3)\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    with Parallel() as parallel:\n",
    "        forest_pred = parallel(delayed(svm_parallel)(X,y \n",
    "                                                     ,X_train,X_test \n",
    "                                                     ,y_train,y_test\n",
    "                                                     ,idx_train,idx_test) for i in range(1,trees))\n",
    "        \n",
    "    # training time\n",
    "    training_time = time.time() - t\n",
    "    \n",
    "    # reshape array such that column k denotes prediction for tree k\n",
    "    forest_pred = np.concatenate(forest_pred,axis=1)\n",
    "    \n",
    "    majority = np.apply_along_axis(majority_vote, 1, forest_pred) \n",
    "    \n",
    "    test = y_test[np.argsort(idx_test)]\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(test, majority)\n",
    "    \n",
    "    return accuracy, training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9631578947368421\n",
      "training time: 1.05 seconds\n"
     ]
    }
   ],
   "source": [
    "# load breast cancer\n",
    "X, y = datasets.load_breast_cancer(return_X_y=True)\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "svm_rf = svm_forest(X,y)\n",
    "print(\"Accuracy: {}\".format(svm_rf[0]))\n",
    "print(\"training time: {0:.2f} seconds\".format(svm_rf[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.589\n",
      "training time: 416.73 seconds\n"
     ]
    }
   ],
   "source": [
    "# load credit card defaults\n",
    "credit = pd.read_csv('credit-card-full.csv', index_col=0)\n",
    "X, y = np.array(credit.iloc[:,:-1]), np.array(credit.iloc[:,-1])\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "svm_rf = svm_forest(X,y)\n",
    "print(\"Accuracy: {}\".format(svm_rf[0]))\n",
    "print(\"training time: {0:.2f} seconds\".format(svm_rf[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
