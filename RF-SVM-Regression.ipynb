{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest support vector regressor (RF-SVR)\n",
    "## Vincent Buekers\n",
    "Promotor: Prof. dr. Johan A.K. Suykens\n",
    "\n",
    "Supervision: Yingyi Chen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import tree, svm, linear_model\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedShuffleSplit\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExtraTree Partition\n",
    "Subsets are obtained from the leaf nodes of an extremely randomized tree. For purposes of theoretical consistency: \n",
    "\n",
    "- only one candidate feature is selected from all d features using the option max_features = 1, yielding totally random trees\n",
    "- the leaf size is set to sqrt(n)\n",
    "\n",
    "Note: these are non-overlapping subsets due to the recursive branching mechanism in decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_partition(X_train,X_test, y_train,y_test, idx_train,idx_test):\n",
    "    \n",
    "    # totally randomized tree (max_features=1)\n",
    "    extra = tree.ExtraTreeRegressor(max_features=1,min_samples_leaf = int(np.sqrt(len(X_train))) )\n",
    "    extra.fit(X_train,y_train)\n",
    "    \n",
    "    # obtain leaf indices the datapoints appear in\n",
    "    leaf_idx_train, leaf_idx_test = extra.apply(X_train), extra.apply(X_test)\n",
    "    \n",
    "    # Keep track of observation indexes and prepare for pandas' .groupby\n",
    "    leaf_idx_train = pd.DataFrame(leaf_idx_train, index=idx_train)\n",
    "    leaf_idx_test = pd.DataFrame(leaf_idx_test, index=idx_test)\n",
    "    \n",
    "    # Group train and test observations by their leaf node\n",
    "    groups_train = leaf_idx_train.groupby(leaf_idx_train[0],axis=0).groups\n",
    "    groups_test = leaf_idx_test.groupby(leaf_idx_test[0],axis=0).groups\n",
    "    \n",
    "    # collect all data back into one array, otherwise leaf indexes will be out of bounds...\n",
    "    X_train, X_test = np.c_[idx_train,X_train], np.c_[idx_test,X_test]\n",
    "    y_train, y_test = np.c_[idx_train,y_train], np.c_[idx_test,y_test]\n",
    "    X, y = np.r_[X_train,X_test], np.r_[y_train,y_test]\n",
    "    # sort by indexes\n",
    "    X, y = X[np.argsort(X[:,0])], y[np.argsort(y[:,0])]\n",
    "    # delete indexes\n",
    "    X, y = np.delete(X, 0, 1), np.delete(y, 0, 1)\n",
    "\n",
    "    subsets_train = {}\n",
    "    subsets_test = {}\n",
    "    \n",
    "    leaf_count = 1\n",
    "    \n",
    "    # Obtain train and test subsets created by the leaf node partitioning\n",
    "    # iterables are a list of Int64index objects for the data in each leaf node\n",
    "    for leaf_train, leaf_test in zip(list(groups_train.values()),list(groups_test.values())) :\n",
    "        \n",
    "        # subset the data\n",
    "        X_train_sub, y_train_sub = X[leaf_train], y[leaf_train]\n",
    "        X_test_sub, y_test_sub = X[leaf_test] ,y[leaf_test]\n",
    "        \n",
    "        # original indexes of the observations appearing in this leaf\n",
    "        train_indexes = np.array(leaf_train).reshape(-1,1)\n",
    "        test_indexes = np.array(leaf_test).reshape(-1,1)\n",
    "        \n",
    "        # training subset including original observation indexes\n",
    "        sub_train = np.c_[train_indexes, X_train_sub, y_train_sub]\n",
    "        # testing subset including original observation indexes and tree predictions\n",
    "        sub_test = np.c_[test_indexes, X_test_sub, y_test_sub]\n",
    "\n",
    "        subsets_train.update({'leaf_'+str(leaf_count):sub_train})\n",
    "        subsets_test.update({'leaf_'+str(leaf_count):sub_test})\n",
    "        \n",
    "        leaf_count +=1\n",
    "    \n",
    "    subsets = {}\n",
    "    subsets.update({'train':subsets_train})\n",
    "    subsets.update({'test':subsets_test})\n",
    "        \n",
    "    return subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVR Training\n",
    "For each leaf an svm regressor is trained on the corresponding subset.\n",
    "\n",
    "- fit_svr_linear: LinearSVC (LibLinear)\n",
    "- fit_svr_sgd: SGDRegressor corresponds to stochastic gradient Linear SVM\n",
    "- fit_svr_kernel: tuned kernel svm\n",
    "\n",
    "The regressors can be tuned by setting tune = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_svr_linear(subset, tune):\n",
    "    \n",
    "    X_train, y_train = subset[:,1:-1], subset[:,-1]\n",
    "        \n",
    "    if tune == True:\n",
    "        \n",
    "        # regularization values\n",
    "        C_range = 2*np.logspace(-5, 3, 15)\n",
    "        param_grid = dict(C=C_range)\n",
    "        \n",
    "        reg = svm.LinearSVR(dual=False)\n",
    "        cv = ShuffleSplit(n_splits=5, test_size=0.2)\n",
    "        tuned = RandomizedSearchCV(reg, param_distributions=param_grid, cv=cv, n_jobs=-1)\n",
    "        tuned.fit(X_train,y_train)\n",
    "        \n",
    "        return tuned\n",
    "    \n",
    "    else:\n",
    "        reg = svm.LinearSVR(dual=False)\n",
    "        reg.fit(X_train,y_train)\n",
    "    \n",
    "        return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_svr_sgd(subset, tune):\n",
    "    \n",
    "    X_train, y_train = subset[:,1:-1], subset[:,-1]\n",
    "    \n",
    "    if tune == True:\n",
    "        tuned = linear_model.SGDRegressor(early_stopping=True, validation_fraction=0.2)\n",
    "        tuned.fit(X_train,y_train)\n",
    "        \n",
    "        return tuned\n",
    "        \n",
    "    else:\n",
    "        reg = linear_model.SGDRegressor()\n",
    "        reg.fit(X_train,y_train)\n",
    "        \n",
    "        return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_svr_kernel(subset, tune):\n",
    "    \n",
    "    X_train, y_train = subset[:,1:-1], subset[:,-1]\n",
    "    \n",
    "    if tune == True:\n",
    "            \n",
    "            C_range = 2*np.logspace(-5, 3, 15)\n",
    "            gamma_range = 2*np.logspace(-15, 3, 3)\n",
    "            kernel_list = ['linear','rbf','poly']\n",
    "            param_grid = dict(gamma=gamma_range, C=C_range, kernel=kernel_list)\n",
    "            \n",
    "            # fit svm to subset\n",
    "            cv = ShuffleSplit(n_splits=5, test_size=0.2)\n",
    "            clf = svm.SVC()\n",
    "            tuned = RandomizedSearchCV(clf, param_distributions=param_grid, cv=cv, n_jobs=-1)\n",
    "            tuned.fit(X_train, y_train)\n",
    "\n",
    "            return tuned\n",
    "        \n",
    "    else:\n",
    "        reg = svm.SVR()\n",
    "        reg.fit(X_train,y_train)\n",
    "  \n",
    "        return reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel SVR training across nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_svr_tree(subsets, variant, tune):\n",
    "    \n",
    "    subsets_train = subsets['train']\n",
    "    # amount of subsets or leaf nodes\n",
    "    leaf_count = subsets_train.keys()\n",
    "    \n",
    "    if variant == \"linear\":\n",
    "        \n",
    "        # Run SVM's in parallel\n",
    "        with Parallel() as parallel:\n",
    "            submodels = parallel(delayed(fit_svr_linear)(subset, tune) for \n",
    "                                 subset in subsets_train.values())\n",
    "    \n",
    "        submodels = dict(zip(leaf_count,submodels))\n",
    "        \n",
    "        return submodels\n",
    "    \n",
    "    elif variant == 'sgd':\n",
    "        \n",
    "        # Run SVM's in parallel\n",
    "        with Parallel() as parallel:\n",
    "            submodels = parallel(delayed(fit_svr_sgd)(subset, tune) for \n",
    "                                 subset in subsets_train.values())\n",
    "    \n",
    "        submodels = dict(zip(leaf_count,submodels))\n",
    "        \n",
    "        return submodels\n",
    "    \n",
    "    elif variant == 'tuned kernel':\n",
    "        \n",
    "        # Run SVM's in parallel\n",
    "        with Parallel() as parallel:\n",
    "            submodels = parallel(delayed(fit_svr_kernel)(subset, tune) for \n",
    "                                 subset in subsets_train.values())\n",
    "    \n",
    "        submodels = dict(zip(leaf_count,submodels))\n",
    "        \n",
    "        return submodels\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_leaf(submodel, subset):  \n",
    "    \n",
    "    idx_test, X_test = subset[:,0], subset[:,1:-1]\n",
    "    \n",
    "    # obtain predictions for subset\n",
    "    pred = submodel.predict(X_test)\n",
    "    # include original observation index\n",
    "    pred = np.concatenate((idx_test.reshape(-1,1), pred.reshape(-1,1)), axis=1)\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_svr_tree(submodels,subsets):\n",
    "    \n",
    "    subsets_test = subsets['test']\n",
    "    \n",
    "    # Predict SVM's in parallel\n",
    "    with Parallel() as parallel:\n",
    "        preds = parallel(delayed(predict_leaf)(submodel, subset) for \n",
    "                             submodel,subset in zip(submodels.values(),subsets_test.values()))\n",
    "       \n",
    "    #aggregate predictions of the leafs into one set of predictions for the tree\n",
    "    preds_all = np.concatenate(preds,axis=0)\n",
    "    \n",
    "    # sort predictions by their index\n",
    "    preds_sorted = preds_all[np.argsort(preds_all[:,0])]\n",
    "    # remove index and reshape\n",
    "    preds_sorted = preds_sorted[:,1].reshape(-1,1)\n",
    "    \n",
    "    return preds_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extend model to ensemble of SVR trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forest_partition(X_train,X_test,y_train,y_test,idx_train,idx_test, n_trees):\n",
    "    \n",
    "    partitions = {}\n",
    "    \n",
    "    for i in range(0,trees):\n",
    "        subsets = extra_partition(X_train,X_test, y_train,y_test, idx_train,idx_test)\n",
    "        partitions.update({'partition_'+str(i): subsets})\n",
    "    \n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_rf_svr(partitions, variant):\n",
    "\n",
    "    forest={}\n",
    "    i = 0\n",
    "    \n",
    "    for partition in partitions.values():\n",
    "        submodels = fit_svr_tree(partition,variant)\n",
    "        forest.update({'tree_'+str(i): submodels})\n",
    "        i += 1\n",
    "        \n",
    "    return forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rf_svr(partitions, forest):\n",
    "    \n",
    "    forest_pred = []\n",
    "    \n",
    "    for partition, SVtree in zip(partitions.values(),forest.values()):\n",
    "        \n",
    "        tree_pred = predict_svr_tree(forest, partition)\n",
    "        forest_pred.append(tree_pred)\n",
    "        \n",
    "    # reshape array such that column k denotes prediction for tree k\n",
    "    forest_pred = np.concatenate(forest_pred,axis=1)\n",
    "    # majority vote\n",
    "    mean_pred = np.apply_along_axis(mean, 1, forest_pred) \n",
    "   \n",
    "    return mean_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
