{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest support vector machine classifier (RF-SVC)\n",
    "## Vincent Buekers\n",
    "Promotor: Prof. dr. Johan A.K. Suykens\n",
    "\n",
    "Supervision: Yingyi Chen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import tree, svm, linear_model\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedShuffleSplit\n",
    "\n",
    "from statistics import mode, StatisticsError\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExtraTree Partition\n",
    "Subsets are obtained from the leaf nodes of an extremely randomized tree. For purposes of theoretical consistency: \n",
    "\n",
    "- only one candidate feature is selected from all d features using the option max_features = 1, yielding totally random trees\n",
    "- the leaf size is set to sqrt(n)\n",
    "\n",
    "Note: these are non-overlapping subsets due to the recursive branching mechanism in decision trees\n",
    "Note2: store extratree predictions to use when leaf node are pure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_partition(X_train,X_test, y_train,y_test, idx_train,idx_test):\n",
    "    \n",
    "    # totally randomized tree (max_features=1)\n",
    "    extra = tree.ExtraTreeClassifier(max_features=1,min_samples_leaf = int(np.sqrt(len(X_train))) )\n",
    "    extra.fit(X_train,y_train)\n",
    "    \n",
    "    # obtain leaf indices the datapoints appear in\n",
    "    leaf_idx_train, leaf_idx_test = extra.apply(X_train), extra.apply(X_test)\n",
    "    \n",
    "    # Keep track of observation indexes and prepare for pandas' .groupby\n",
    "    leaf_idx_train = pd.DataFrame(leaf_idx_train, index=idx_train)\n",
    "    leaf_idx_test = pd.DataFrame(leaf_idx_test, index=idx_test)\n",
    "    \n",
    "    # Group train and test observations by their leaf node\n",
    "    groups_train = leaf_idx_train.groupby(leaf_idx_train[0],axis=0).groups\n",
    "    groups_test = leaf_idx_test.groupby(leaf_idx_test[0],axis=0).groups\n",
    "    \n",
    "    # collect all data back into one array, sorted by original observation indexes\n",
    "    X_train, X_test = np.c_[idx_train,X_train], np.c_[idx_test,X_test]\n",
    "    y_train, y_test = np.c_[idx_train,y_train], np.c_[idx_test,y_test]\n",
    "    X, y = np.r_[X_train,X_test], np.r_[y_train,y_test]\n",
    "    X, y = X[np.argsort(X[:,0])], y[np.argsort(y[:,0])]\n",
    "    X, y = np.delete(X, 0, 1), np.delete(y, 0, 1)\n",
    "    \n",
    "    # tree predictions (only test observations will be retrieved later on)\n",
    "    preds_tree = extra.predict(X).reshape(-1,1)\n",
    "    \n",
    "    subsets_train = {}\n",
    "    subsets_test = {}\n",
    "    \n",
    "    leaf_count = 1\n",
    "    \n",
    "    # Obtain train and test subsets created by the leaf node partitioning\n",
    "    # iterables are a list of Int64index objects for the data in each leaf node\n",
    "    for leaf_train, leaf_test in zip(list(groups_train.values()),list(groups_test.values())) :\n",
    "        \n",
    "        # subset the data\n",
    "        X_train_sub, y_train_sub = X[leaf_train], y[leaf_train]\n",
    "        X_test_sub, y_test_sub, y_tree = X[leaf_test] ,y[leaf_test], preds_tree[leaf_test]\n",
    "        \n",
    "        # original indexes of the observations appearing in this leaf\n",
    "        train_indexes = np.array(leaf_train).reshape(-1,1)\n",
    "        test_indexes = np.array(leaf_test).reshape(-1,1)\n",
    "        \n",
    "        # training subset including original observation indexes\n",
    "        sub_train = np.c_[train_indexes, X_train_sub, y_train_sub]\n",
    "        # testing subset including original observation indexes and tree predictions\n",
    "        sub_test = np.c_[test_indexes, X_test_sub, y_test_sub, y_tree]\n",
    "\n",
    "        subsets_train.update({'leaf_'+str(leaf_count):sub_train})\n",
    "        subsets_test.update({'leaf_'+str(leaf_count):sub_test})\n",
    "        \n",
    "        leaf_count +=1\n",
    "    \n",
    "    subsets = {}\n",
    "    subsets.update({'train':subsets_train})\n",
    "    subsets.update({'test':subsets_test})\n",
    "        \n",
    "    return subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedded SVM classifiers\n",
    "for each subset an svm classifier is trained on the training subset and used to predict the corresponding leaf test test (if the leaf is not yet homogenous in terms of class labels). \n",
    "\n",
    "- fit_svc_linear: LinearSVC (LibLinear)\n",
    "- fit_svc_sgd: SGDClassifier \n",
    "- fit_svc_kernel: tuned kernel svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_svc_linear(subset):\n",
    "    \n",
    "    X_train, y_train = subset[:,1:-1], subset[:,-1]\n",
    "    \n",
    "    # check if leaf node is heterogeneous (i.e. consists of more than one class) \n",
    "    # also check if it contains enough samples to conduct training (2)\n",
    "    if len(np.unique(y_train)) >= 2 and (np.bincount(y_train.astype(int)) >= 2).all():\n",
    "        \n",
    "        # decide whether to solve in primal or dual\n",
    "        QP_bool = False if (X_train.shape[0] > X_train.shape[0]) else True\n",
    "        \n",
    "        # regularization values\n",
    "        C_range = np.logspace(-2, 10, 13)\n",
    "        param_grid = dict(C=C_range)\n",
    "        \n",
    "        # fit svm to subset\n",
    "        cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2)\n",
    "        clf = svm.LinearSVC(class_weight='balanced', dual=QP_bool)\n",
    "        tuned = RandomizedSearchCV(clf, param_distributions=param_grid, cv=cv, n_jobs=-1)\n",
    "        tuned.fit(X_train,y_train)\n",
    "\n",
    "        return tuned\n",
    "    \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_svc_sgd(subset):\n",
    "    \n",
    "    X_train, y_train = subset[:,1:-1], subset[:,-1]\n",
    "        \n",
    "    # check if leaf node is heterogeneous (i.e. consists of more than one class) \n",
    "    # also check if it contains enough samples to conduct training (2)\n",
    "    if len(np.unique(y_train)) >= 2 and (np.bincount(y_train.astype(int)) >= 2).all():\n",
    "        \n",
    "        # fit sgd classifier\n",
    "        # validation_fraction=0.2 = 5-fold cross-validation\n",
    "        # balanced class_weight = class-specific regularization\n",
    "        clf = linear_model.SGDClassifier(class_weight='balanced',\n",
    "                                         early_stopping=True,\n",
    "                                         validation_fraction=0.2)\n",
    "        clf.fit(X_train,y_train)\n",
    "\n",
    "        return clf\n",
    "    \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_svc_tuned(subset):\n",
    "    \n",
    "    X_train, y_train = subset[:,1:-1], subset[:,-1]\n",
    "    \n",
    "    C_range = np.logspace(-2, 10, 13)\n",
    "    gamma_range = np.logspace(-9, 3, 13)\n",
    "    kernel_list = ['linear','rbf','poly']\n",
    "    param_grid = dict(gamma=gamma_range, C=C_range, kernel=kernel_list)\n",
    "        \n",
    "    # check if leaf node is heterogeneous (i.e. consists of more than one class) \n",
    "    # also check if it contains enough samples to conduct training (2)\n",
    "    if len(np.unique(y_train)) >= 2 and (np.bincount(y_train.astype(int)) >= 2).all():\n",
    "\n",
    "        # fit svm to subset\n",
    "        cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2)\n",
    "        clf = svm.SVC(class_weight='balanced')\n",
    "        tuned = RandomizedSearchCV(clf, param_distributions=param_grid, cv=cv, n_jobs=-1)\n",
    "        tuned.fit(X_train, y_train)\n",
    "        \n",
    "        #print(tuned.best_params_)\n",
    "\n",
    "        return tuned\n",
    "    \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel training across nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_svc_tree(subsets, variant):\n",
    "    \n",
    "    # retrieve training subsets\n",
    "    subsets_train = subsets['train']\n",
    "    # amount of subsets or leaf nodes\n",
    "    leaf_count = subsets_train.keys()\n",
    "    \n",
    "    if variant == \"linear\":\n",
    "        \n",
    "        # Run SVM's in parallel\n",
    "        with Parallel() as parallel:\n",
    "            submodels = parallel(delayed(fit_svc_linear)(subset) for subset in subsets_train.values())\n",
    "    \n",
    "        submodels = dict(zip(leaf_count,submodels))\n",
    "        \n",
    "        return submodels\n",
    "    \n",
    "    elif variant == 'sgd':\n",
    "        \n",
    "        # Run SVM's in parallel\n",
    "        with Parallel() as parallel:\n",
    "            submodels = parallel(delayed(fit_svc_sgd)(subset) for subset in subsets_train.values())\n",
    "    \n",
    "        submodels = dict(zip(leaf_count,submodels))\n",
    "        \n",
    "        return submodels\n",
    "    \n",
    "    elif variant == 'tuned kernel':\n",
    "        \n",
    "        # Run SVM's in parallel\n",
    "        with Parallel() as parallel:\n",
    "            submodels = parallel(delayed(fit_svc_tuned)(subset) for subset in subsets_train.values())\n",
    "    \n",
    "        submodels = dict(zip(leaf_count,submodels))\n",
    "        \n",
    "        return submodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_leaf(submodel, subset):  \n",
    "    \n",
    "    idx_test, X_test, y_tree = subset[:,0], subset[:,1:-2], subset[:,-1]\n",
    "    \n",
    "    # use tree predictions if leaf node is pure\n",
    "    if submodel==None:\n",
    "        tree_pred = np.concatenate((idx_test.reshape(-1,1), y_tree.reshape(-1,1)), axis=1)\n",
    "        return tree_pred\n",
    "    \n",
    "    else:    \n",
    "        # obtain predictions for subset\n",
    "        svm_pred = submodel.predict(X_test)\n",
    "        # include original observation index\n",
    "        svm_pred = np.concatenate((idx_test.reshape(-1,1), svm_pred.reshape(-1,1)), axis=1)\n",
    "        \n",
    "        return svm_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_svc_tree(submodels,subsets):\n",
    "    \n",
    "    # retrieve test subsets\n",
    "    subsets_test = subsets['test']\n",
    "    \n",
    "    # Predict SVM's in parallel\n",
    "    with Parallel() as parallel:\n",
    "        preds = parallel(delayed(predict_leaf)(submodel, subset) for \n",
    "                             submodel,subset in zip(submodels.values(),subsets_test.values()))\n",
    "       \n",
    "    #aggregate predictions of the leafs into one set of predictions for the tree\n",
    "    preds_all = np.concatenate(preds,axis=0)\n",
    "    \n",
    "    # sort predictions by their index\n",
    "    preds_sorted = preds_all[np.argsort(preds_all[:,0])]\n",
    "    # remove index and reshape\n",
    "    preds_sorted = preds_sorted[:,1].reshape(-1,1)\n",
    "    \n",
    "    return preds_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF-SVC ensemble\n",
    "Finally, the above procedure can be extended to an ensemble of randomized trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forest_partition(X_train,X_test,y_train,y_test,idx_train,idx_test, n_trees):\n",
    "    \n",
    "    partitions = {}\n",
    "    \n",
    "    for i in range(0,trees):\n",
    "        subsets = extra_partition(X_train,X_test, y_train,y_test, idx_train,idx_test)\n",
    "        partitions.update({'partition_'+str(i): subsets})\n",
    "    \n",
    "    return partitions    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_rf_svc(partitions, variant):\n",
    "\n",
    "    forest={}\n",
    "    i = 0\n",
    "    \n",
    "    for partition in partitions.values():\n",
    "        submodels = fit_svc_tree(partition,variant)\n",
    "        forest.update({'tree_'+str(i): submodels})\n",
    "        i += 1\n",
    "        \n",
    "    return forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ensemble prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain majority vote for each datapoint\n",
    "def majority_vote(l):\n",
    "    try:\n",
    "        return mode(l)\n",
    "    except StatisticsError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rf_svc(forest, partitions):\n",
    "    \n",
    "    forest_pred = []\n",
    "    \n",
    "    for submodels, subsets in zip(forest.values(),partitions.values()):\n",
    "        \n",
    "        tree_pred = predict_svc_tree(submodels, subsets)\n",
    "        forest_pred.append(tree_pred)\n",
    "        \n",
    "    # reshape array such that column k denotes prediction for tree k\n",
    "    forest_pred = np.concatenate(forest_pred,axis=1)\n",
    "    # majority vote\n",
    "    majority = np.apply_along_axis(majority_vote, 1, forest_pred) \n",
    "   \n",
    "    return majority.reshape(-1,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
