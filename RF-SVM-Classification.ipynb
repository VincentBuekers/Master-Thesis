{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest support vector machine classifier (RF-SVC)\n",
    "## Vincent Buekers\n",
    "Promotor: Prof. dr. Johan A.K. Suykens\n",
    "\n",
    "Supervision: Yingyi Chen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import ensemble, svm, linear_model, datasets\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, StratifiedShuffleSplit, train_test_split\n",
    "\n",
    "from statistics import mode, StatisticsError\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExtraTree Partition\n",
    "leafs are obtained from the leaf nodes of an extremely randomized tree. For purposes of theoretical consistency: \n",
    "\n",
    "- only one candidate feature is selected from all d features using the option max_features = 1, yielding totally random trees\n",
    "- the leaf size is set to sqrt(n)\n",
    "\n",
    "Note: these are non-overlapping leafs due to the recursive branching mechanism in decision trees\n",
    "Note2: store extratree predictions to use when leaf node are pure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_partition(X_train, X_test, y_train, n_estimators=10):\n",
    "    \n",
    "    # totally randomized forest (max_features=1)\n",
    "    extra = ensemble.ExtraTreesClassifier(n_estimators=n_estimators,\n",
    "                                          max_features=1, \n",
    "                                          min_samples_leaf = int(np.sqrt(len(X_train))), \n",
    "                                          n_jobs=-1, \n",
    "                                          class_weight=\"balanced\", \n",
    "                                          )\n",
    "    \n",
    "    extra.fit(X_train,y_train)\n",
    "    y_pred_extra = extra.predict(X_test)\n",
    "\n",
    "    leaf_idx_train = pd.DataFrame(extra.apply(X_train))\n",
    "    leaf_idx_test = pd.DataFrame(extra.apply(X_test))\n",
    "\n",
    "    partitions={}\n",
    "    \n",
    "    for k in leaf_idx_train.columns:\n",
    "    \n",
    "        leafs_train = leaf_idx_train.groupby(leaf_idx_train[k],axis=0).groups\n",
    "        leafs_test = leaf_idx_test.groupby(leaf_idx_test[k],axis=0).groups\n",
    "\n",
    "        partition={}\n",
    "        \n",
    "        i=0\n",
    "    \n",
    "        for leaf_train, leaf_test in zip(leafs_train.values(), leafs_test.values()):\n",
    "        \n",
    "            X_train_sub, y_train_sub = X_train[leaf_train], y_train[leaf_train]\n",
    "            X_test_sub = X_test[leaf_test]\n",
    "            y_pred_sub = y_pred_extra[leaf_test]\n",
    "    \n",
    "            leaf={}\n",
    "        \n",
    "            leaf.update({'X_train':X_train_sub,\n",
    "                         'y_train':y_train_sub, \n",
    "                         'X_test':X_test_sub, \n",
    "                         'index_test': leaf_test,\n",
    "                         'y_pred_extra':y_pred_sub})\n",
    "        \n",
    "            partition.update({\"leaf_\"+str(i):leaf})\n",
    "            i+=1\n",
    "        \n",
    "        partitions.update({\"tree_\"+str(k):partition})\n",
    "        \n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedded SVM classifiers\n",
    "for each leaf an svm classifier is trained on the training leaf and used to predict the corresponding leaf test test (if the leaf is not yet homogenous in terms of class labels). \n",
    "\n",
    "- fit_svc_linear: LinearSVC (LibLinear)\n",
    "- fit_svc_sgd: SGDClassifier \n",
    "\n",
    "Whether the classifiers should be tuned or not can be specified by setting tune = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_svc_linear(leaf, tune):\n",
    "    \n",
    "    X_train, y_train = leaf[\"X_train\"], leaf[\"y_train\"]\n",
    "    \n",
    "    # check if leaf node is heterogeneous (i.e. consists of more than one class) \n",
    "    # also check if it contains enough samples to conduct training (2)\n",
    "    if len(np.unique(y_train)) >= 2 and (np.bincount(y_train.astype(int)) >= 2).all():\n",
    "        \n",
    "        # decide whether to solve in primal or dual\n",
    "        QP_bool = False if (X_train.shape[0] > X_train.shape[1]) else True\n",
    "        \n",
    "        if tune == True:\n",
    "            \n",
    "            # regularization values\n",
    "            C_range = np.logspace(-1,1,6)\n",
    "            grid = dict(C=C_range)\n",
    "        \n",
    "            # fit svm to leaf\n",
    "            cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2)\n",
    "            clf = svm.LinearSVC(class_weight='balanced', dual=QP_bool)\n",
    "            tuned = GridSearchCV(clf, param_grid= grid, cv=cv, n_jobs=-1)\n",
    "            tuned.fit(X_train,y_train)\n",
    "\n",
    "            return tuned\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            clf = svm.LinearSVC(class_weight='balanced', dual=QP_bool)\n",
    "            clf.fit(X_train,y_train)\n",
    "            \n",
    "            return clf\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_svc_sgd(leaf, tune):\n",
    "    \n",
    "    X_train, y_train = leaf[\"X_train\"], leaf[\"y_train\"]\n",
    "    n = X_train.shape[0]\n",
    "    \n",
    "    # check if leaf node is heterogeneous (i.e. consists of more than one class) \n",
    "    # also check if it contains enough samples to conduct training (2)\n",
    "    if len(np.unique(y_train)) >= 2 and (np.bincount(y_train.astype(int)) >= 2).all():\n",
    "        \n",
    "        if tune == True:\n",
    "            \n",
    "            alpha_range=10.0**-np.arange(1,7)\n",
    "            grid = dict(alpha=alpha_range)\n",
    "            \n",
    "            cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2)\n",
    "            clf = linear_model.SGDClassifier(class_weight='balanced', early_stopping=False, max_iter = np.ceil(10**6 / n))\n",
    "            \n",
    "            tuned = GridSearchCV(clf, param_grid=grid, cv=cv, n_jobs=-1)\n",
    "            tuned.fit(X_train,y_train)\n",
    "\n",
    "            return tuned\n",
    "        \n",
    "        else:\n",
    "            clf = linear_model.SGDClassifier(class_weight=\"balanced\")\n",
    "            clf.fit(X_train,y_train)\n",
    "\n",
    "            return clf\n",
    "            \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel training across nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_svc_tree(leafs, variant, tune):\n",
    "    \n",
    "    # amount of leafs or leaf nodes\n",
    "    leaf_count = leafs.keys()\n",
    "    \n",
    "    if variant == \"linear\":\n",
    "        \n",
    "        # Run SVM's in parallel\n",
    "        with Parallel() as parallel:\n",
    "            leaf_models = parallel(delayed(fit_svc_linear)(leaf, tune) for leaf in leafs.values())\n",
    "    \n",
    "        leaf_models = dict(zip(leaf_count,leaf_models))\n",
    "        \n",
    "        return leaf_models\n",
    "    \n",
    "    elif variant == \"sgd\":\n",
    "        \n",
    "        # Run SVM's in parallel\n",
    "        with Parallel() as parallel:\n",
    "            leaf_models = parallel(delayed(fit_svc_sgd)(leaf, tune) for leaf in leafs.values())\n",
    "    \n",
    "        leaf_models = dict(zip(leaf_count,leaf_models))\n",
    "        \n",
    "        return leaf_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel training across trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_rf_svc(partitions, variant, tune):\n",
    "    \n",
    "    tree_count = partitions.keys()\n",
    "    \n",
    "    with Parallel() as parallel:\n",
    "        forest = parallel(delayed(fit_svc_tree)(partition,variant, tune) for partition in partitions.values())\n",
    "        \n",
    "    forest = dict(zip(tree_count, forest))\n",
    "        \n",
    "    return forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_leaf(leaf, leaf_model):  \n",
    "    \n",
    "    idx_test, X_test, y_pred_extra = leaf[\"index_test\"], leaf[\"X_test\"], leaf[\"y_pred_extra\"], \n",
    "    \n",
    "    # use tree predictions if leaf node is pure\n",
    "    if leaf_model == None:\n",
    "        pred = dict(zip(idx_test,y_pred_extra))\n",
    "        return pred\n",
    "    \n",
    "    else:    \n",
    "        # obtain predictions for leaf\n",
    "        pred = leaf_model.predict(X_test)\n",
    "        # include original observation index\n",
    "        pred = dict(zip(idx_test, pred))\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tree(leafs,leaf_models):\n",
    "    \n",
    "    # Predict SVM's in parallel\n",
    "    with Parallel() as parallel:\n",
    "        preds = parallel(delayed(predict_leaf)(leaf, leaf_model) for leaf,leaf_model in zip(leafs.values(),leaf_models.values()))\n",
    "    \n",
    "    preds_all = {}\n",
    "    for leaf_preds in preds:\n",
    "        preds_all.update(leaf_preds)\n",
    "        \n",
    "    preds_sorted=[]\n",
    "    for key in sorted(preds_all):\n",
    "        preds_sorted.append(preds_all[key])\n",
    "    \n",
    "    return np.array(preds_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain majority vote for each datapoint\n",
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)\n",
    "\n",
    "def majority_vote(l):\n",
    "    try:\n",
    "        return mode(l)\n",
    "    except StatisticsError:\n",
    "        return most_common(list(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_forest(partitions,forest):\n",
    "    \n",
    "    forest_pred = []\n",
    "    \n",
    "    for leafs, leaf_models in zip(partitions.values(),forest.values()):\n",
    "        \n",
    "        tree_pred = predict_tree(leafs, leaf_models)\n",
    "        tree_pred = tree_pred.reshape(-1,1)\n",
    "        forest_pred.append(tree_pred)\n",
    "        \n",
    "    # reshape array such that column k denotes prediction for tree k\n",
    "    forest_pred = np.concatenate(forest_pred,axis=1)\n",
    "    # majority vote\n",
    "    majority = np.apply_along_axis(majority_vote, 1, forest_pred) \n",
    "   \n",
    "    return majority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
